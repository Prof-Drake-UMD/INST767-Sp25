# -*- coding: utf-8 -*-
"""issue1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PDSQA8YTsWS5TI3HRu50lZ_e8XectG-s

# **Issue 1:**
By now you should be set up and have this repository cloned and connected. Make your first commit by creating a README.md file. If you need a refresher on markdown go here. Edit the file to list the API data sources you intend to integrate. You should include any inputs to the APIs and the expected data that will come back.

Then, write some python code to make the api calls and pull the data down. These can be functions in the same .py file or separate files in a module. Either is fine. The functions should hit the api, gather the data that comes back, and have the JSON ready to be placed somewhere. Think of this as the beginning of the ingest phase.
"""

!pip install requests

# -*- coding: utf-8 -*-
"""issue1_finalv.py

Updated for schema flattening and CSV output.

Original file is located at
    https://colab.research.google.com/drive/1PDSQA8YTsWS5TI3HRu50lZ_e8XectG-s

"""

import requests
import pandas as pd
import numpy as np
import os
import json
import random
import time
from datetime import datetime
import base64
import pprint
import re

client_id = "653aec7ed52e453a9db9eb5796a0f306"
client_secret = "68983ccf13a7463f91f7c636f04cff21"

def get_spotify_token(client_id, client_secret):
    auth_url = "https://accounts.spotify.com/api/token"
    headers = {
        "Authorization": "Basic " + base64.b64encode(f"{client_id}:{client_secret}".encode()).decode()
    }
    data = {
        "grant_type": "client_credentials"
    }
    response = requests.post(auth_url, headers=headers, data=data)
    response.raise_for_status()
    return response.json()['access_token']

spotify_token = get_spotify_token(client_id, client_secret)

def get_book_by_keyword(title, author):
    query = f"{title} {author}"
    search_url = f"https://openlibrary.org/search.json?q={query}"
    resp = requests.get(search_url)
    docs = resp.json().get("docs", [])

    for book in docs[:15]:
        book_title = book.get("title", "").lower()
        authors = [a.lower() for a in book.get("author_name", [])]
        publish_year = book.get("first_publish_year")
        work_key = book.get("key")
        languages = book.get("language", [])

        if (
            title.lower() in book_title and
            any(author.lower() in a for a in authors) and
            "eng" in languages and
            publish_year and publish_year >= 1950 and
            work_key
        ):
            return {
                "book_id": work_key,
                "title": book.get("title"),
                "author_name": book.get("author_name", [None])[0],
                "first_publish_year": publish_year,
                "language": "eng",
                "book_url": f"https://openlibrary.org{work_key}",
                "ingest_ts": datetime.utcnow().isoformat()
            }

    return None

def get_met_artworks_by_year_range(year, buffer=10):
    search_url = "https://collectionapi.metmuseum.org/public/collection/v1/search"
    object_url = "https://collectionapi.metmuseum.org/public/collection/v1/objects/"
    results = []
    for y in range(year - buffer, year + buffer + 1):
        resp = requests.get(search_url, params={"q": str(y), "hasImages": True})
        ids = resp.json().get("objectIDs", [])
        if ids:
            random.shuffle(ids)
            for object_id in ids[:5]:
                obj_data = requests.get(object_url + str(object_id)).json()
                obj_year = obj_data.get("objectBeginDate", 0)
                if obj_data and 1950 <= obj_year <= 2025 and abs(obj_year - year) <= buffer:
                    results.append({
                        "object_id": obj_data.get("objectID"),
                        "title": obj_data.get("title"),
                        "artist_name": obj_data.get("artistDisplayName"),
                        "object_date": obj_data.get("objectDate"),
                        "medium": obj_data.get("medium"),
                        "image_url": obj_data.get("primaryImageSmall"),
                        "object_url": obj_data.get("objectURL"),
                        "ingest_ts": datetime.utcnow().isoformat()
                    })
    return results

def get_spotify_tracks_by_year(year, token, limit=10):
    search_url = "https://api.spotify.com/v1/search"
    headers = {
        "Authorization": f"Bearer {token}"
    }
    results = []
    latin_char_results = []

    for attempt in range(3):
        params = {
            "q": f"year:{year}",
            "type": "track",
            "limit": 50
        }
        resp = requests.get(search_url, headers=headers, params=params)
        if resp.status_code == 200:
            data = resp.json().get("tracks", {}).get("items", [])
            for track in data:
                track_data = {
                    "track_id": track["id"],
                    "title": track["name"],
                    "artist": track["artists"][0]["name"],
                    "album": track["album"]["name"],
                    "release_date": track["album"]["release_date"],
                    "preview_url": track["preview_url"],
                    "ingest_ts": datetime.utcnow().isoformat()
                }

                if re.match(r'^[A-Za-z0-9\s\.,!?\'"()\-\[\]:;]+$', track["name"]):
                    latin_char_results.append(track_data)
                else:
                    results.append(track_data)
            break

    if len(latin_char_results) >= limit:
        return latin_char_results[:limit]
    else:
        return latin_char_results + results[:max(0, limit - len(latin_char_results))]

def match_cultural_experience_by_year(book_title, author_name, limit=5, year_buffer=10):
    book = get_book_by_keyword(book_title, author_name)
    if not book:
        return {"error": "Book not found."}

    year = book.get("first_publish_year")
    artworks = get_met_artworks_by_year_range(year, buffer=year_buffer)
    music = get_spotify_tracks_by_year(year, spotify_token, limit=limit * 2)

    return {
        "step": "year_only",
        "book": book,
        "artworks": artworks[:limit],
        "music": music[:limit]
    }

def flatten_and_export(result, output_csv='output/cultural_experience_items.csv'):
    os.makedirs(os.path.dirname(output_csv), exist_ok=True)
    rows = []
    book = result.get('book', {})
    artworks = result.get('artworks', [])
    music = result.get('music', [])
    match_type = result.get('step', None)
    ingest_ts = book.get('ingest_ts')  # or use datetime.utcnow().isoformat() for all

    # Ensure at least one row, use max length of artworks/music or 1 if both empty
    max_len = max(len(artworks), len(music), 1)
    for i in range(max_len):
        art = artworks[i] if i < len(artworks) else {}
        mus = music[i] if i < len(music) else {}

        row = {
            "book_id": book.get("book_id"),
            "book_title": book.get("title"),
            "book_author": book.get("author_name"),
            "book_first_publish_year": book.get("first_publish_year"),
            "book_language": book.get("language"),
            "book_url": book.get("book_url"),
            "object_id": art.get("object_id"),
            "artwork_title": art.get("title"),
            "artwork_artist": art.get("artist_name"),
            "artwork_medium": art.get("medium"),
            "artwork_date": art.get("object_date"),
            "artwork_url": art.get("object_url"),
            "artwork_image_url": art.get("image_url"),
            "track_id": mus.get("track_id"),
            "track_title": mus.get("title"),
            "track_artist": mus.get("artist"),
            "album_title": mus.get("album"),
            "track_release_date": mus.get("release_date"),
            "track_preview_url": mus.get("preview_url"),
            "match_type": match_type,
            "ingest_ts": ingest_ts
        }
        rows.append(row)
    df = pd.DataFrame(rows)
    df.to_csv(output_csv, index=False)
    print(f"Exported {len(rows)} rows to {output_csv}")


result = match_cultural_experience_by_year("Normal People", "Sally Rooney")
pprint.pprint(result)

# Export to CSV for BigQuery
flatten_and_export(result)
